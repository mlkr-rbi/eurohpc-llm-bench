#!/bin/bash

# copy to run.slurm, edit the parameters if required,
# and run: sbatch run.slurm

# SLURM job directives
## account-related setup
#SBATCH --account=ehpc124              # Project account
#SBATCH --partition=EuroHPC            # max. nodes (cores), wallclock: 100 (8000),	72h
#SBATCH --qos=acc_ehpc                 # Quality of Service (QoS) level
## acc_ehpc is the "queue" we need to use, its params are here:
## https://www.bsc.es/supportkc/docs/MareNostrum5/slurm/#queues-qos

## fixed params
#SBATCH --job-name=my_job_name         # Name of the job
#SBATCH --output=slurm/output.%j.log   # Standard output file (with job ID)
#SBATCH --error=slurm/error.%j.log     # Standard error file (with job ID)
#SBATCH --mail-type=none               # {begin|end|all|none}
#SBATCH --mail-user=user@example.com   # Email for notifications
## ntasks needs to be 1 for deepspeed: https://huggingface.co/docs/transformers/deepspeed
#SBATCH --ntasks-per-node=1            # Number of tasks per node

## resources, below is the test setup, change as needed for production
## effective batch size is nodes * gpus * batch_per_gpu (huggingface param)
#SBATCH --nodes=2                     # Number of nodes required
#SBATCH --cpus-per-task=16             # Number of CPUs per task
#SBATCH --gres=gpu:4                   # Number of GPUs per node, max. 4 for marenostrum5
## max. 72hrs for acc_ehpc
#SBATCH --time=00:15:00                # Maximum runtime (HH:MM:SS)


ROOT_DIR="/gpfs/projects/ehpc124"

# Load required modules
module purge
module load bsc
source $ROOT_DIR/scripts/support-scripts/load-modules-py311-5.sh

# Set environment variables if needed
## for deepspeed
export MASTER_ADDR=$(scontrol show hostnames $SLURM_JOB_NODELIST | head -n 1)
export MASTER_PORT=$(( RANDOM % 10000 + 30000 ))  # Random port between 30000-39999

# experiment (learning etc.), and deepspeed configuration files
EXPERIMENT_FILE="mt-train-2b-wiki-hr-deepspeed.yml"
DEEPSPEED_CONFIG="experiments/deepspeed/deepspeed_test.json"

# run the job
srun --nodes=1 --ntasks=1 python main.py --experiment $EXPERIMENT_FILE --cached_tokenization --tokenize_only
srun deepspeed main.py --deepspeed $DEEPSPEED_CONFIG --experiment $EXPERIMENT_FILE --cached_tokenization