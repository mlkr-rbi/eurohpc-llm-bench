model_id: "HuggingFaceTB/SmolLM-135M"
model_label: "SmolLM-135M"
dataset_label: "test_cro"
output_dir_tag: "model_SmolLM-135M"
logging_dir_tag: "logs_SmolLM-135M"
num_train_epochs: 0.05
gradient_accumulation_steps: 8
per_device_train_batch_size: 4
per_device_eval_batch_size: 4
logging_steps: 10
eval_strategy: "no"
eval_steps: -1
save_steps: 500
save_total_limit: 2
learning_rate: 0.0002
weight_decay: 0.001
max_grad_norm: 0.3
warmup_ratio: 0.03
fp16: False
bf16: True
remove_unused_columns: False
ddp_find_unused_parameters: False
gradient_checkpointing: True
# QUANTIZATION AND PEFT
quantize:
  enabled: True
  load_in_4bit: True
  bnb_4bit_quant_type: "nf4"
  bnb_4bit_compute_dtype: "torch.bfloat16"
  bnb_4bit_use_double_quant: False
peft:
  enabled: True
  r: 32
  lora_alpha: 32
  target_modules: ["q_proj", "k_proj"]
  lora_dropout: 0.05
  bias: "none"
  task_type: "CAUSAL_LM"