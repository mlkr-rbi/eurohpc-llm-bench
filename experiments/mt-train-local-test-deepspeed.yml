# test for running on local (weak) machines - setup with a small transformer

action: training # Start training action
model_id: "HuggingFaceTB/SmolLM-135M"
model_label: "SmolLM-135M"
dataset_label: "/data/datasets/corpora/classla/balkan-wikis/classla_hr_wiki_huggingface_docs_sample/"
output_dir_tag: "model_SmolLM-135M"
logging_dir_tag: "logs_SmolLM-135M"
max_seq_length: 256
num_train_epochs: 3
gradient_accumulation_steps: 8
per_device_train_batch_size: 4
per_device_eval_batch_size: 4
logging_steps: 10
eval_strategy: "no"
eval_steps: -1
save_steps: 500
save_total_limit: 2
learning_rate: 0.0001
weight_decay: 0.001
max_grad_norm: 0.3
warmup_ratio: 0.03
fp16: False
bf16: True
remove_unused_columns: False
ddp_find_unused_parameters: False
gradient_checkpointing: True
# DEEPSPEED
# put .json block here, it has to be indented, and '|' means take the block as literal string
# these settings need to be synced with the other training and quantization parameters
deepspeed: |
  {
    "zero_optimization": {
      "stage": 2,
      "offload_optimizer": {
        "device": "none"
      },
      "overlap_comm": true,
      "contiguous_gradients": true
    },
    "fp16": {
      "enabled": false
    },
    "bf16": {
      "enabled": true
    },
    "train_micro_batch_size_per_gpu": 4
  }
# QUANTIZATION AND PEFT
quantize:
  enabled: True
  load_in_4bit: True
  bnb_4bit_quant_type: "nf4"
  bnb_4bit_compute_dtype: "torch.bfloat16"
  bnb_4bit_use_double_quant: False
peft:
  enabled: True
  r: 32
  lora_alpha: 32
  target_modules: ["q_proj", "k_proj"]
  lora_dropout: 0.05
  bias: "none"
  task_type: "CAUSAL_LM"